{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef031375-6aff-4b99-9b07-37c576f92e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install highway-env\n",
    "# !pip install git+https://github.com/DLR-RM/stable-baselines3\n",
    "# !pip install tensorboardx gym pyvirtualdisplay\n",
    "# !apt-get install -y xvfb python-opengl ffmpeg\n",
    "# !git clone https://github.com/eleurent/highway-env.git 2> /dev/null\n",
    "# !git clone https://github.com/avivg7/highway-config.git\n",
    "# !pip install tensorboard\n",
    "# !pip install xvfbwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6779162-b029-4026-b66d-cb325698adea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.7.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import sys\n",
    "sys.path.insert(0, '/content/highway-env/scripts/')\n",
    "import io\n",
    "import base64\n",
    "import os\n",
    "from tqdm.notebook import trange\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pygame\n",
    "import json\n",
    "import ast\n",
    "import pprint \n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "# Local\n",
    "from final_project.display_utils import wrap_env, show_video\n",
    "from models.agent import Agent\n",
    "from models.utils import plot_learning_curve\n",
    "\n",
    "# Gym Env\n",
    "import gym\n",
    "import highway_env\n",
    "# from utils import record_videos, show_videos\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "from gym.utils import seeding\n",
    "from gym import error, spaces, utils\n",
    "gymlogger.set_level(40) # error only\n",
    "\n",
    "# Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a794abc-5326-49a0-b30f-f3cd6e9d40d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7aeeb5-bfd6-4abe-9ba1-10781109dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97a80b27-236d-4ded-aa73-d4b13ae15f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5047:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2565:(snd_pcm_open_noupdate) Unknown PCM default\n",
      "ALSA lib confmisc.c:767:(parse_card) cannot find card '0'\n",
      "ALSA lib conf.c:4568:(_snd_config_evaluate) function snd_func_card_driver returned error: No such file or directory\n",
      "ALSA lib confmisc.c:392:(snd_func_concat) error evaluating strings\n",
      "ALSA lib conf.c:4568:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\n",
      "ALSA lib confmisc.c:1246:(snd_func_refer) error evaluating name\n",
      "ALSA lib conf.c:4568:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\n",
      "ALSA lib conf.c:5047:(snd_config_expand) Evaluate error: No such file or directory\n",
      "ALSA lib pcm.c:2565:(snd_pcm_open_noupdate) Unknown PCM default\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'best_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27719/3655693344.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m               max_size=10000)\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mbest_score\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_range\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mscore_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mload_cehcpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_score' is not defined"
     ]
    }
   ],
   "source": [
    "#=============== DO NOT DELETE ===============\n",
    "# file = open('/content/highway-config/config_ex1.txt', 'r')\n",
    "# contents = file.read()\n",
    "# config1 = ast.literal_eval(contents)\n",
    "# file.close()\n",
    "# ============================================\n",
    "\n",
    "file = open('./highway-config/config_ex1.txt', 'r')\n",
    "contents = file.read()\n",
    "# print(contents)\n",
    "config1 = ast.literal_eval(contents)\n",
    "file.close()\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\")\n",
    "env.configure(config1)\n",
    "# env = wrap_env(env)\n",
    "env.reset()\n",
    "done = False\n",
    "n_games = 500\n",
    "figure_file = 'plots/highway_500'\n",
    "\n",
    "agent = Agent(input_dims=env.observation_space.shape,\n",
    "              env=env,\n",
    "              n_actions=5,\n",
    "              max_size=10000)\n",
    "\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "load_cehcpoint = False\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "    env.render(mode='human')\n",
    "    \n",
    "for i in range(n_games):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.remember(observation, action, reward, new_observation, done)\n",
    "        if not load_checkpoint:\n",
    "            agent.learn()\n",
    "        observation = new_observation\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "    \n",
    "    screen = env.render(mode='rgb_array')\n",
    "    plt.imshow(screen)\n",
    "    print(f'Episode: {i}, Score {score}, Average Score: {avg_score}')\n",
    "    \n",
    "if not load_checkpoint:\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    plot_learning_curve(x, score_history, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7743b86c-61ec-42ef-953d-bb0744002314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(5)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f08b8ba-9594-4501-ae2f-d5d6b4cbdb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find video\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6685347f-05c1-43bf-bb44-6a48bf98b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.agent import Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7dd900-f1ec-4329-84bd-8ece5dc41419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "    def init_weights(m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            torch.nn.init.uniform(m.weight,  -3*1.e-4, 3*1.e-4)\n",
    "            m.bias.data.fill_(0.0001)\n",
    "\n",
    "class ActorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.shared_layers = nn.Sequential( # todo: name\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(start_dim=0),\n",
    "            nn.Linear(64*12*12, 5),\n",
    "        )\n",
    "        self.shared_layers.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = torch.tensor(x, dtype=torch.float)\n",
    "        hidden = self.shared_layers(input_tensor)\n",
    "        #output = self.output(hidden)\n",
    "        actions_probs = Categorical(F.softmax(hidden, dim=-1))\n",
    "\n",
    "        return actions_probs, hidden\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CriticNet, self).__init__()\n",
    "        self.shared_layers = nn.Sequential(  # todo: name\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(start_dim=0),\n",
    "            nn.Linear(64 * 12 * 12, 1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_tensor = torch.FloatTensor(x)\n",
    "        hidden = self.shared_layers(input_tensor)\n",
    "\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43edbf6b-8770-4cd1-b5b3-7a34bd7c7b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %load_ext tensorboard\n",
    "# %matplotlib inline\n",
    "\n",
    "# =============== DO NOT DELETE ===============\n",
    "file = open('../highway-config/config_ex1.txt', 'r')\n",
    "contents = file.read()\n",
    "config1 = ast.literal_eval(contents)\n",
    "file.close()\n",
    "# ============================================\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\")\n",
    "config1['duration']= 500\n",
    "env.configure(config1)\n",
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.3f}\".format(x)})\n",
    "\n",
    "obs = env.reset()\n",
    "for j in range(10):\n",
    "    obs, _, _, _ = env.step(0)\n",
    "\n",
    "    _, axes = plt.subplots(ncols=4, figsize=(12, 5))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(obs[i, ...].T, cmap=plt.get_cmap('gray'))\n",
    "\n",
    "env = wrap_env(env)\n",
    "observation = env.reset()\n",
    "done = False\n",
    "iter = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## play single env\n",
    "\n",
    "def main():\n",
    "\n",
    "    actor_net  = ActorNet()\n",
    "    critic_net = CriticNet()\n",
    "    #\n",
    "    curr_state = env.reset()\n",
    "    #state_t = curr_state.transpose((1,2,0))\n",
    "\n",
    "\n",
    "    optimizer_actor = optim.Adam(actor_net.parameters(), lr=0.0001) # todo: which optimize\n",
    "    optimizer_critic = optim.Adam(critic_net.parameters(), lr=0.0001) # todo: which optimize\n",
    "    # optimizer.zero_grad()\n",
    "    # actor_net.eval() # from here\n",
    "    for i in range(500):\n",
    "        single_episode(actor_net, critic_net, env, episode_num=i)\n",
    "        if i % 4 == 0:\n",
    "            train(optimizer_actor, optimizer_critic)\n",
    "\n",
    "def train(optimizer_actor, optimizer_critic):\n",
    "    global REPLAY_VALUES, REPLAY_REWARDS, REPLAY_NEXT_VALUES, REPLAY_RETURNS, REPLAY_LOG_PROBS\n",
    "    current_buffer_size = len(REPLAY_RETURNS)\n",
    "    if current_buffer_size < 50:\n",
    "        print('buffer size is too small:', current_buffer_size)\n",
    "        return\n",
    "\n",
    "    chosen_indexes = random.sample(range(current_buffer_size), 32)\n",
    "\n",
    "    log_probs = REPLAY_LOG_PROBS[chosen_indexes]\n",
    "    returns = REPLAY_RETURNS[chosen_indexes]\n",
    "    values = REPLAY_VALUES[chosen_indexes]\n",
    "    rewards = REPLAY_REWARDS[chosen_indexes]\n",
    "    next_values = REPLAY_NEXT_VALUES[chosen_indexes]\n",
    "\n",
    "    critic_loss = F.mse_loss(values, (rewards + 0.99 * next_values)) # todo: validate why 0.999\n",
    "\n",
    "    actor_loss = -(log_probs * (returns - values)).mean() #-(log_probs * advantage.detach()).mean()\n",
    "    # actor_loss = log_prob * advantage\n",
    "    #\n",
    "    #\n",
    "    optimizer_actor.zero_grad()\n",
    "    optimizer_critic.zero_grad()\n",
    "    critic_loss.backward(retain_graph=True)\n",
    "    print(f'action loss: {actor_loss.item()}, critic_loss:{critic_loss.item()}')\n",
    "    actor_loss.backward() # todo\n",
    "\n",
    "    optimizer_actor.step()\n",
    "    optimizer_critic.step()\n",
    "\n",
    "    REPLAY_VALUES = torch.FloatTensor()\n",
    "    REPLAY_REWARDS = torch.FloatTensor()\n",
    "    REPLAY_NEXT_VALUES = torch.FloatTensor()\n",
    "    REPLAY_RETURNS = torch.FloatTensor()\n",
    "    REPLAY_LOG_PROBS = torch.FloatTensor()\n",
    "\n",
    "def compute_returns(next_value, rewards, masks, values, discount_factor=0.99):\n",
    "    # \"update V using target r+...)\n",
    "    returns = []\n",
    "    R = next_value\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + (discount_factor * R ) * masks[step] # todo: in some version it without \"- values[step]\"\n",
    "        # next_value_ = values[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns\n",
    "\n",
    "REPLAY_VALUES = torch.FloatTensor()\n",
    "REPLAY_REWARDS = torch.FloatTensor()\n",
    "REPLAY_NEXT_VALUES = torch.FloatTensor()\n",
    "REPLAY_RETURNS = torch.FloatTensor()\n",
    "REPLAY_LOG_PROBS = torch.FloatTensor()\n",
    "#assert len(torch_values)==  len(torch_rewards) == len(torch_next_values) == len(torch_asa)\n",
    "\n",
    "EPSILON_GREEDY_VALUE = 0.6\n",
    "\n",
    "# def _choose_action_use_epsinlon_greedy(actions_probs):\n",
    "#     if random.uniform(0, 1) <= EPSILON_GREEDY_VALUE:\n",
    "#         return actions_probs.sample()\n",
    "#     rand_index = random.randint(0,3)\n",
    "#     return actions_probs[rand_index]\n",
    "\n",
    "\n",
    "def single_episode(actor_net, critic_net, env, episode_num):\n",
    "    global REPLAY_VALUES, REPLAY_REWARDS, REPLAY_NEXT_VALUES, REPLAY_RETURNS, REPLAY_LOG_PROBS\n",
    "    # print('start training epoch ' , epoch)\n",
    "    curr_state = env.reset()\n",
    "    entropy = 0 # why this\n",
    "    log_probs = []\n",
    "    values = []\n",
    "    v_pi_hat = []\n",
    "    rewards = []\n",
    "    masks = [] # i don't know this\n",
    "    done = False\n",
    "    steps = 0\n",
    "    actions_counter = Counter()\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        # 1. take action\n",
    "        (actions_probs, actions_return), value = actor_net(curr_state), critic_net(curr_state)\n",
    "        print(f\"probs {actions_probs.probs.detach().numpy()} returns {actions_return}\")\n",
    "        # action = _choose_action_use_epsinlon_greedy(actions_probs)\n",
    "        action = actions_probs.sample()\n",
    "        actions_counter[action.item()] +=1\n",
    "        next_state, reward, done, extra_info = env.step(action.item())\n",
    "        if episode_num>100 and episode_num % 20 == 0:\n",
    "            screen = env.render(mode='rgb_array')\n",
    "            plt.imshow(screen)\n",
    "\n",
    "        log_prob = actions_probs.log_prob(action).unsqueeze(0) # math.log(actions_probs.probs[action]) # ==\n",
    "        entropy += actions_probs.entropy().mean() # why this\n",
    "\n",
    "        log_probs.append(log_prob)\n",
    "        values.append(value)\n",
    "        rewards.append(torch.tensor([reward], dtype=torch.float))\n",
    "        masks.append(torch.tensor([1 - done], dtype=torch.float))\n",
    "        curr_state = next_state # need?\n",
    "        if done:\n",
    "            print(f'epoch: {episode_num}, Steps: {steps} , extra-info:{extra_info} , '\n",
    "                  f'{actions_probs.probs.detach().numpy()}, actions-counter:{actions_counter}')\n",
    "            break\n",
    "\n",
    "\n",
    "    next_state = torch.FloatTensor(next_state)\n",
    "    next_value = critic_net(next_state)\n",
    "\n",
    "\n",
    "    ### update V\n",
    "    #critic_loss = F.mse_loss(torch.cat(values), (torch.cat(rewards) + 0.9 * torch.cat(values[1:] + [next_value])))\n",
    "    # critic_loss = (torch.cat(rewards) + 0.9 * torch.cat(values)).mean()\n",
    "    ###\n",
    "\n",
    "    # Evaluate\n",
    "    returns = compute_returns(next_value, rewards, masks, values)\n",
    "\n",
    "    torch_values = torch.cat(values)\n",
    "    torch_rewards = torch.cat(rewards)\n",
    "    torch_next_values = torch.cat(values[1:] + [next_value])\n",
    "    torch_returns = torch.cat(returns)\n",
    "    torch_log_probs = torch.cat(log_probs)\n",
    "\n",
    "    assert len(torch_values)==  len(torch_rewards) == len(torch_next_values) == len(torch_returns) == len(torch_log_probs)\n",
    "\n",
    "    REPLAY_VALUES = torch.cat((REPLAY_VALUES, torch_values), 0)\n",
    "    REPLAY_REWARDS = torch.cat((REPLAY_REWARDS, torch_rewards), 0)\n",
    "    REPLAY_NEXT_VALUES = torch.cat((REPLAY_NEXT_VALUES, torch_next_values), 0)\n",
    "    REPLAY_RETURNS = torch.cat((REPLAY_RETURNS, torch_returns), 0)\n",
    "    REPLAY_LOG_PROBS = torch.cat((REPLAY_LOG_PROBS, torch_log_probs), 0)\n",
    "\n",
    "\n",
    "    #\n",
    "    # log_probs = torch.cat(log_probs) # np.array(log_probs)\n",
    "    # returns = torch.cat(returns).detach() #torch.cat(returns).detach()\n",
    "    # values = torch.cat(values) #torch.cat(values)\n",
    "    #\n",
    "    # advantage = returns - values\n",
    "    # \"evaluate A...\"\n",
    "    # returns = torch.cat(rewards) + advantage\n",
    "\n",
    "\n",
    "# 2. update V (critic)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# while (iter < 10) or not done:\n",
    "#     if done:\n",
    "#         break\n",
    "#     iter += 1\n",
    "#     action = env.action_space.sample()\n",
    "#     observation, reward, done, _ = env.step(action)\n",
    "#     screen = env.render(mode='rgb_array')\n",
    "#     plt.imshow(screen)\n",
    "#     print(f'iteration: {iter}, action: {action}, reward: {reward}, done: {done}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
