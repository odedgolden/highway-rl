{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef031375-6aff-4b99-9b07-37c576f92e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install highway-env\n",
    "# !pip install git+https://github.com/DLR-RM/stable-baselines3\n",
    "# !pip install tensorboardx gym pyvirtualdisplay\n",
    "# !apt-get install -y xvfb python-opengl ffmpeg\n",
    "# !git clone https://github.com/eleurent/highway-env.git 2> /dev/null\n",
    "# !git clone https://github.com/avivg7/highway-config.git\n",
    "# !pip install tensorboard\n",
    "# !pip install xvfbwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c2a559-404e-4a2d-b709-a65972057e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/cffi/cparser.py:165: UserWarning: Global variable 'stderr' in cdef(): for consistency with C it should have a storage class specifier (usually 'extern')\n",
      "  \"(usually 'extern')\" % (decl.name,))\n",
      "/opt/conda/lib/python3.7/site-packages/cffi/cparser.py:165: UserWarning: Global variable '__stderrp' in cdef(): for consistency with C it should have a storage class specifier (usually 'extern')\n",
      "  \"(usually 'extern')\" % (decl.name,))\n"
     ]
    }
   ],
   "source": [
    "from cffi import FFI\n",
    "import os\n",
    "\n",
    "ffi = FFI()\n",
    "ffi.cdef(\"\"\"\n",
    "/* from stdio.h */\n",
    "FILE* fopen(const char* path, const char* mode);\n",
    "int fclose(FILE* fp);\n",
    "FILE* stderr;  /* GNU C library */\n",
    "FILE* __stderrp;  /* Mac OS X */\n",
    "\"\"\")\n",
    "try:\n",
    "    stdio = ffi.dlopen(None)\n",
    "    devnull = stdio.fopen(os.devnull.encode(), b'w')\n",
    "except OSError:\n",
    "    pass\n",
    "try:\n",
    "    stdio.stderr = devnull\n",
    "except KeyError:\n",
    "    try:\n",
    "        stdio.__stderrp = devnull\n",
    "    except KeyError:\n",
    "        stdio.fclose(devnull)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6779162-b029-4026-b66d-cb325698adea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.16, Python 3.7.12)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import sys\n",
    "sys.path.insert(0, '/content/highway-env/scripts/')\n",
    "import io\n",
    "import base64\n",
    "import os\n",
    "from tqdm.notebook import trange\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import pygame\n",
    "import json\n",
    "import ast\n",
    "from pprint import pprint \n",
    "import glob\n",
    "from collections import Counter\n",
    "\n",
    "# Local\n",
    "from final_project.display_utils import wrap_env, show_video\n",
    "from models.agent import Agent\n",
    "from models.utils import plot_learning_curve\n",
    "\n",
    "# Gym Env\n",
    "import gym\n",
    "import highway_env\n",
    "# from utils import record_videos, show_videos\n",
    "from gym import logger as gymlogger\n",
    "from gym.wrappers import Monitor\n",
    "from gym.utils import seeding\n",
    "from gym import error, spaces, utils\n",
    "gymlogger.set_level(40) # error only\n",
    "\n",
    "# Neural Networks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e7aeeb5-bfd6-4abe-9ba1-10781109dbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff54115e-c798-4170-883e-01bc0a09e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #=============== DO NOT DELETE ===============\n",
    "# file = open('./highway-config/config_ex1.txt', 'r')\n",
    "# contents = file.read()\n",
    "# config1 = ast.literal_eval(contents)\n",
    "# file.close()\n",
    "# # ============================================\n",
    "\n",
    "# env = gym.make(\"highway-fast-v0\")\n",
    "# env.configure(config1)\n",
    "\n",
    "# obs = env.reset()\n",
    "# for j in range(10):\n",
    "#     obs, _, _, _ = env.step(0)\n",
    "\n",
    "#     _, axes = plt.subplots(ncols=4, figsize=(12, 5))\n",
    "#     for i, ax in enumerate(axes.flat):\n",
    "#         ax.imshow(obs[i, ...].T, cmap=plt.get_cmap('gray'))\n",
    "    \n",
    "\n",
    "# env = wrap_env(env)\n",
    "# env.reset()\n",
    "# done = False\n",
    "# iter = 0\n",
    "\n",
    "# while (iter < 10) or not done:\n",
    "#   if done:\n",
    "#     break\n",
    "#   iter +=1\n",
    "#   action = env.action_space.sample()\n",
    "#   observation, reward, done, _ = env.step(action)\n",
    "#   screen = env.render(mode='rgb_array')\n",
    "#   plt.imshow(screen)\n",
    "#   print(f'iteration: {iter}, action: {action}, reward: {reward}, done: {done}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a794abc-5326-49a0-b30f-f3cd6e9d40d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c18e2d7-f457-4c3a-b84e-fb851f1cf429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ...,  0.,  0.,  0.]],\n",
       "\n",
       "       [[99., 99., 99., ..., 99., 99., 99.],\n",
       "        [99., 99., 99., ..., 99., 99., 99.],\n",
       "        [99., 99., 99., ..., 99., 99., 99.],\n",
       "        ...,\n",
       "        [99., 99., 99., ..., 99., 99., 99.],\n",
       "        [99., 99., 99., ..., 99., 99., 99.],\n",
       "        [99., 99., 99., ..., 99., 99., 99.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#=============== DO NOT DELETE ===============\n",
    "# file = open('/content/highway-config/config_ex1.txt', 'r')\n",
    "# contents = file.read()\n",
    "# config1 = ast.literal_eval(contents)\n",
    "# file.close()\n",
    "# ============================================\n",
    "\n",
    "file = open('./highway-config/config_ex1.txt', 'r')\n",
    "contents = file.read()\n",
    "# print(contents)\n",
    "config1 = ast.literal_eval(contents)\n",
    "file.close()\n",
    "\n",
    "env = gym.make(\"highway-fast-v0\")\n",
    "env.configure(config1)\n",
    "# env = wrap_env(env)\n",
    "env.reset()\n",
    "# env.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb5e244b-e18b-4ec3-967d-26f2b25508dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.max(observation)\n",
    "\n",
    "# while a == 0:\n",
    "#     action = env.action_space.sample()\n",
    "#     observation, reward, done, _ = env.step(action)\n",
    "#     a = np.max(observation)\n",
    "# print(f'np.max(observation): {np.max(observation)}')\n",
    "# print(f'np.min(observation): {np.min(observation)}')    \n",
    "# for i in range(100):\n",
    "#     action = env.action_space.sample()\n",
    "#     observation, reward, done, _ = env.step(action)\n",
    "\n",
    "#     pprint(action)\n",
    "# # pprint(observation)\n",
    "#     print(f'np.max(observation): {np.max(observation)}')\n",
    "#     print(f'np.min(observation): {np.min(observation)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c06e2ed1-c479-47ad-bbee-a1a4d1f2aa8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 128, 128)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d919a47-82ed-45a0-a69c-6af8f90ef0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a80b27-236d-4ded-aa73-d4b13ae15f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 4, 128, 128)\n",
      "4 128 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/highway-rl/models/agent.py:58: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755953518/work/torch/csrc/utils/tensor_new.cpp:210.)\n",
      "  state = T.Tensor([observation]).to(self.actor.device)\n",
      "/home/jupyter/highway-rl/models/networks.py:138: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  prob = F.softmax(prob)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Best Score 12.861835182390749, Average Score: 12.861835182390749, Top Score: 12.861835182390749, Steps: 17\n",
      "action_history: (array([0.35294118, 0.17647059, 0.23529412, 0.23529412]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 1, Best Score 12.861835182390749, Average Score: 10.547116319718533, Top Score: 12.861835182390749, Steps: 10\n",
      "action_history: (array([0.33333333, 0.11111111, 0.22222222, 0.33333333]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 2, Best Score 12.861835182390749, Average Score: 10.091186547796937, Top Score: 12.861835182390749, Steps: 11\n",
      "action_history: (array([0.28947368, 0.10526316, 0.21052632, 0.39473684]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 3, Best Score 12.861835182390749, Average Score: 9.025864851737289, Top Score: 12.861835182390749, Steps: 7\n",
      "action_history: (array([0.26666667, 0.08888889, 0.24444444, 0.4       ]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 4, Best Score 12.861835182390749, Average Score: 11.07241875156356, Top Score: 19.258634350868643, Steps: 27\n",
      "action_history: (array([0.27777778, 0.16666667, 0.22222222, 0.33333333]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 5, Best Score 12.861835182390749, Average Score: 10.720041502146984, Top Score: 19.258634350868643, Steps: 11\n",
      "action_history: (array([0.27710843, 0.1686747 , 0.24096386, 0.31325301]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 6, Best Score 12.861835182390749, Average Score: 12.470895821981697, Top Score: 22.976021740989985, Steps: 32\n",
      "action_history: (array([0.28695652, 0.16521739, 0.2173913 , 0.33043478]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 7, Best Score 12.861835182390749, Average Score: 11.780363614423353, Top Score: 22.976021740989985, Steps: 9\n",
      "action_history: (array([0.2983871 , 0.15322581, 0.20967742, 0.33870968]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 8, Best Score 12.861835182390749, Average Score: 11.608430370648339, Top Score: 22.976021740989985, Steps: 14\n",
      "action_history: (array([0.2826087 , 0.15217391, 0.19565217, 0.36956522]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 9, Best Score 13.737256548301065, Average Score: 13.737256548301065, Top Score: 32.8966921471756, Steps: 45\n",
      "action_history: (array([0.28415301, 0.1420765 , 0.19672131, 0.37704918]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 10, Best Score 13.8565208368992, Average Score: 13.8565208368992, Top Score: 32.8966921471756, Steps: 17\n",
      "action_history: (array([0.295, 0.145, 0.195, 0.365]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 11, Best Score 13.8565208368992, Average Score: 13.698397966879488, Top Score: 32.8966921471756, Steps: 14\n",
      "action_history: (array([0.29906542, 0.14953271, 0.19626168, 0.35514019]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 12, Best Score 13.8565208368992, Average Score: 13.704095187858258, Top Score: 32.8966921471756, Steps: 16\n",
      "action_history: (array([0.28695652, 0.16086957, 0.2       , 0.35217391]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 13, Best Score 13.8565208368992, Average Score: 13.596469782575891, Top Score: 32.8966921471756, Steps: 16\n",
      "action_history: (array([0.28861789, 0.15447154, 0.19918699, 0.35772358]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 14, Best Score 14.422529652056939, Average Score: 14.422529652056939, Top Score: 32.8966921471756, Steps: 33\n",
      "action_history: (array([0.27598566, 0.15412186, 0.21505376, 0.35483871]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 15, Best Score 15.074054918881714, Average Score: 15.074054918881714, Top Score: 32.8966921471756, Steps: 31\n",
      "action_history: (array([0.25483871, 0.16451613, 0.21935484, 0.36129032]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 16, Best Score 15.074054918881714, Average Score: 14.871580024567296, Top Score: 32.8966921471756, Steps: 15\n",
      "action_history: (array([0.24923077, 0.17230769, 0.21846154, 0.36      ]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 17, Best Score 16.29069610406681, Average Score: 16.29069610406681, Top Score: 40.4156694555586, Steps: 51\n",
      "action_history: (array([0.2393617 , 0.18351064, 0.20212766, 0.375     ]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 18, Best Score 16.481195019948835, Average Score: 16.481195019948835, Top Score: 40.4156694555586, Steps: 25\n",
      "action_history: (array([0.23441397, 0.18204489, 0.19950125, 0.3840399 ]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 19, Best Score 16.481195019948835, Average Score: 16.305497422287925, Top Score: 40.4156694555586, Steps: 16\n",
      "action_history: (array([0.23021583, 0.18465228, 0.19904077, 0.38609113]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 20, Best Score 16.59257000123635, Average Score: 16.59257000123635, Top Score: 40.4156694555586, Steps: 26\n",
      "action_history: (array([0.23024831, 0.18284424, 0.20316027, 0.38374718]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 21, Best Score 16.59257000123635, Average Score: 16.16080312475538, Top Score: 40.4156694555586, Steps: 8\n",
      "action_history: (array([0.22616408, 0.18181818, 0.20399113, 0.38802661]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 22, Best Score 16.59257000123635, Average Score: 16.135926032739256, Top Score: 40.4156694555586, Steps: 20\n",
      "action_history: (array([0.22080679, 0.18046709, 0.20169851, 0.3970276 ]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 23, Best Score 16.59257000123635, Average Score: 15.755081064775586, Top Score: 40.4156694555586, Steps: 9\n",
      "action_history: (array([0.21875   , 0.18333333, 0.2       , 0.39791667]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 24, Best Score 16.59257000123635, Average Score: 15.465655601018407, Top Score: 40.4156694555586, Steps: 11\n",
      "action_history: (array([0.21792261, 0.18737271, 0.19551935, 0.39918534]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 25, Best Score 16.59257000123635, Average Score: 15.202760092380977, Top Score: 40.4156694555586, Steps: 10\n",
      "action_history: (array([0.21756487, 0.18762475, 0.19560878, 0.3992016 ]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 26, Best Score 16.59257000123635, Average Score: 14.983693929438703, Top Score: 40.4156694555586, Steps: 12\n",
      "action_history: (array([0.21832359, 0.18518519, 0.19883041, 0.39766082]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 27, Best Score 16.59257000123635, Average Score: 14.673188120525925, Top Score: 40.4156694555586, Steps: 8\n",
      "action_history: (array([0.22072937, 0.18426104, 0.19769674, 0.39731286]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 28, Best Score 16.59257000123635, Average Score: 14.998630069905978, Top Score: 40.4156694555586, Steps: 31\n",
      "action_history: (array([0.2192029 , 0.1865942 , 0.19384058, 0.40036232]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 29, Best Score 16.59257000123635, Average Score: 14.63200906757578, Top Score: 40.4156694555586, Steps: 6\n",
      "action_history: (array([0.21863799, 0.18996416, 0.19354839, 0.39784946]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 30, Best Score 16.59257000123635, Average Score: 14.50265865084713, Top Score: 40.4156694555586, Steps: 12\n",
      "action_history: (array([0.21578947, 0.1877193 , 0.19649123, 0.4       ]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 31, Best Score 16.59257000123635, Average Score: 15.0474282095085, Top Score: 40.4156694555586, Steps: 44\n",
      "action_history: (array([0.21824104, 0.18241042, 0.19869707, 0.40065147]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 32, Best Score 16.59257000123635, Average Score: 15.82958463390073, Top Score: 40.8585902144521, Steps: 54\n",
      "action_history: (array([0.21856287, 0.18413174, 0.19461078, 0.40269461]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 33, Best Score 16.59257000123635, Average Score: 15.885083665128986, Top Score: 40.8585902144521, Steps: 20\n",
      "action_history: (array([0.21511628, 0.18604651, 0.20348837, 0.39534884]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 34, Best Score 16.59257000123635, Average Score: 15.859644737113133, Top Score: 40.8585902144521, Steps: 16\n",
      "action_history: (array([0.21164773, 0.18465909, 0.20738636, 0.39630682]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 35, Best Score 16.59257000123635, Average Score: 15.731254134456286, Top Score: 40.8585902144521, Steps: 15\n",
      "action_history: (array([0.21140473, 0.1821975 , 0.20723227, 0.39916551]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 36, Best Score 16.59257000123635, Average Score: 15.47939456240398, Top Score: 40.8585902144521, Steps: 9\n",
      "action_history: (array([0.21016484, 0.17994505, 0.20741758, 0.40247253]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 37, Best Score 16.59257000123635, Average Score: 15.337167111968478, Top Score: 40.8585902144521, Steps: 12\n",
      "action_history: (array([0.21351351, 0.17837838, 0.20810811, 0.4       ]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 38, Best Score 16.59257000123635, Average Score: 15.755616495832674, Top Score: 40.8585902144521, Steps: 41\n",
      "action_history: (array([0.21510883, 0.17797695, 0.2099872 , 0.39692702]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 39, Best Score 16.59257000123635, Average Score: 15.505081176189629, Top Score: 40.8585902144521, Steps: 7\n",
      "action_history: (array([0.21573604, 0.17639594, 0.2106599 , 0.39720812]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 40, Best Score 16.59257000123635, Average Score: 15.441273828534458, Top Score: 40.8585902144521, Steps: 16\n",
      "action_history: (array([0.21268657, 0.1778607 , 0.21144279, 0.39800995]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 41, Best Score 16.59257000123635, Average Score: 15.681209583172771, Top Score: 40.8585902144521, Steps: 33\n",
      "action_history: (array([0.21027479, 0.17801673, 0.2078853 , 0.40382318]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 42, Best Score 16.59257000123635, Average Score: 15.411136197263172, Top Score: 40.8585902144521, Steps: 5\n",
      "action_history: (array([0.20902613, 0.17933492, 0.20665083, 0.40498812]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 43, Best Score 16.59257000123635, Average Score: 15.530545415365934, Top Score: 40.8585902144521, Steps: 24\n",
      "action_history: (array([0.20669746, 0.17667436, 0.20323326, 0.41339492]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 44, Best Score 16.59257000123635, Average Score: 15.316531475589692, Top Score: 40.8585902144521, Steps: 7\n",
      "action_history: (array([0.20618557, 0.17525773, 0.20389462, 0.41466208]), array([0, 1, 2, 3, 4]))\n",
      "\n",
      "Episode: 45, Best Score 16.59257000123635, Average Score: 15.879127727709186, Top Score: 41.19595907308644, Steps: 50\n",
      "action_history: (array([0.20585049, 0.17659805, 0.20585049, 0.41170098]), array([0, 1, 2, 3, 4]))\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_games = 500\n",
    "figure_file = 'plots/highway_500'\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "# print(env.observation_space.shape)\n",
    "agent = Agent(input_dims=env.observation_space.shape,\n",
    "              env=env,\n",
    "              n_actions=5,\n",
    "              max_size=10000)\n",
    "\n",
    "best_score = env.reward_range[0]\n",
    "top_score = env.reward_range[0]\n",
    "score_history = []\n",
    "load_checkpoint = False\n",
    "action_history = []\n",
    "\n",
    "if load_checkpoint:\n",
    "    agent.load_models()\n",
    "    env.render(mode='human')\n",
    "    \n",
    "for i in range(n_games):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    steps = 0\n",
    "    while not done:\n",
    "        steps += 1\n",
    "        action = agent.choose_action(observation)\n",
    "        action_history.append(action)\n",
    "        salt = np.random.rand()\n",
    "        if salt > 0.15:\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            # if action == 4 and salt < 0.95:\n",
    "            #     reward = 0.8*reward\n",
    "        else:\n",
    "            new_observation, reward, done, info = env.step(env.action_space.sample())\n",
    "        score += reward\n",
    "        # print(f'observation.shape: {observation.shape}')\n",
    "        agent.remember(observation, action, reward, new_observation, done)\n",
    "        if not load_checkpoint:\n",
    "            agent.learn()\n",
    "        observation = new_observation\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "    \n",
    "    top_score = max(score, top_score)\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        if not load_checkpoint:\n",
    "            agent.save_models()\n",
    "    \n",
    "    screen = env.render(mode='rgb_array')\n",
    "    plt.imshow(screen)\n",
    "    print(f'Episode: {i}, Best Score {best_score}, Average Score: {avg_score}, Top Score: {top_score}, Steps: {steps}')\n",
    "    print(f'action_history: {np.histogram(action_history, bins=np.arange(5), density=True)}\\n')\n",
    "    \n",
    "if not load_checkpoint:\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    plot_learning_curve(x, score_history, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7743b86c-61ec-42ef-953d-bb0744002314",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not load_checkpoint:\n",
    "    x = [i+1 for i in range(n_games)]\n",
    "    plot_learning_curve(x, score_history, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08b8ba-9594-4501-ae2f-d5d6b4cbdb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.close()\n",
    "# show_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5914c78-bcd4-498e-be74-5bb539a436ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f3a22e-742e-4a2e-8cf7-5b3e4360ac58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-11.m94",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-11:m94"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
